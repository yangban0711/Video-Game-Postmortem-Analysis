{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6004dd7f-4fd6-4db9-8c4d-ccf1cb0a9c06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job done-\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import mean_squared_error, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.svm import SVR, SVC\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "print('Job done-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b86a908a-fcd9-4fdb-978e-67235f9592ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.svm import SVR\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "import string\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f139de76-bcb4-42ce-8d21-6e169c4970e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3680d03c-6cdc-46e5-8af6-da7a9f646cff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs finished b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hooni/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "total_file_path = \"data/dataset.txt\"\n",
    "total_documents = []\n",
    "total_document = []\n",
    "\n",
    "with open(total_file_path, 'r', encoding='utf-8') as file:\n",
    "    # Postmortum 읽기\n",
    "    for line in file:\n",
    "        # Document separation\n",
    "        if line == '\\n':\n",
    "            total_documents.append('\\n'.join(total_document))\n",
    "            total_document = []\n",
    "\n",
    "        # Add to document\n",
    "        total_document.append(line)\n",
    "\n",
    "    # For last game document\n",
    "    total_documents.append('\\n'.join(total_document))\n",
    "\n",
    "total_documents = np.array(total_documents).reshape(-1, 1)\n",
    "    \n",
    "# 목표값 붙이기\n",
    "file_path = 'data/classification.txt'\n",
    "class_list = []\n",
    "\n",
    "class_size = 28   ### 29 -> 28\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        class_type = str(line.strip())\n",
    "        class_list.append(class_type)\n",
    "        \n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "le = LabelEncoder()\n",
    "le_label = le.fit_transform(np.array(class_list)).reshape(-1, 1)\n",
    "ohe_label = ohe.fit_transform(np.array(class_list).reshape(-1, 1))\n",
    "\n",
    "label_size = le_label.shape[1] + ohe_label.shape[1]\n",
    "data_index = np.arange(1, total_documents.shape[0] + 1).reshape(-1, 1)\n",
    "data = np.hstack((data_index, total_documents, le_label, ohe_label))\n",
    "\n",
    "print('jobs finished b')\n",
    "\n",
    "# 데이터셋 분할\n",
    "test_ratio = 0.2\n",
    "index_size = 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[:, :-label_size],\n",
    "                                                    data[:, -label_size:],\n",
    "                                                    test_size=test_ratio,\n",
    "                                                    stratify=data[:, -label_size:-label_size + le_label.shape[1]],\n",
    "                                                    random_state=0)\n",
    "\n",
    "# 자료형 변환\n",
    "X_train_index, X_test_index = X_train[:, :index_size].astype(int), X_test[:, :index_size].astype(int)\n",
    "X_train, X_test = X_train[:, index_size:], X_test[:, index_size:]\n",
    "X_train = X_train.flatten()\n",
    "X_test = X_test.flatten()\n",
    "y_train_ohe = y_train[:, le_label.shape[1]:].astype(np.float64)\n",
    "y_test_ohe = y_test[:, le_label.shape[1]:].astype(np.float64)\n",
    "y_train_le = y_train[:, :le_label.shape[1]].astype(np.float64)\n",
    "y_test_le = y_test[:, :le_label.shape[1]].astype(np.float64)\n",
    "\n",
    "# Result Preparation\n",
    "result_table = pd.DataFrame(columns=['index', 'type', 'model', 'truth', 'prediction', 'f1', 'auc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f0ea7f-43c4-48ba-b684-4659d032190f",
   "metadata": {},
   "source": [
    "#### 2. 워드 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7902916-1955-4e7b-9000-8d934790496a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=(534, 4513)\n",
      "X_test.shape=(134, 4513)\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF 학습\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "print(f'{X_train.shape=}')\n",
    "\n",
    "# 검증 데이터 워드 임베딩\n",
    "X_test = tfidf_vectorizer.transform(X_test).toarray()\n",
    "print(f'{X_test.shape=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1511c9-f3de-4294-9f42-12ecdba3ff36",
   "metadata": {},
   "source": [
    "#### 3. 차원 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49c7d789-e2e1-4c76-97b0-88f8163f58da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced Dimension: 4513 -> 440\n"
     ]
    }
   ],
   "source": [
    "pca_var_ratio = 0.95\n",
    "pca = PCA(n_components=pca_var_ratio)\n",
    "original_dimension = X_train.shape[1]\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "print(f'Reduced Dimension: {original_dimension} -> {X_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c7e6b-1bbc-4c63-86c1-2df1e2aa63a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4. Model Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba228f6-51cf-4f3a-a0c4-bd5ea475095f",
   "metadata": {},
   "source": [
    "##### 4-3. rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c4a31a-8600-446d-906c-b9fb3fb85f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multiclass-multioutput is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 정확도 및 분류 보고서 출력\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n\u001b[1;32m     25\u001b[0m auc_score \u001b[38;5;241m=\u001b[39m roc_auc_score(y_test, y_pred, multi_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# 'ovr'은 One-vs-Rest 방식을 사용함\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[1;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    202\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:106\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    109\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[0;31mValueError\u001b[0m: multiclass-multioutput is not supported"
     ]
    }
   ],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(data[:, :vector_size + label_size],\n",
    " #                                                   data[:, vector_size + label_size],\n",
    "  #                                                  test_size=test_ratio,\n",
    "   #                                                 stratify=data[:, vector_size + label_size],\n",
    "    #                                                random_state=0)\n",
    "\n",
    "\n",
    "# 필요한 라이브러리 임포트\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Random Forest 모델 생성 및 학습\n",
    "\n",
    "rf_model = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 테스트 데이터에 대한 예측\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# 정확도 및 분류 보고서 출력\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc_score = roc_auc_score(y_test, y_pred, multi_class='ovr')  # 'ovr'은 One-vs-Rest 방식을 사용함\n",
    "print(f'AUC: {auc_score:0.3f}')\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a84df6b-e736-4ee2-b81c-04581ed36c10",
   "metadata": {},
   "source": [
    "##### 4-4. xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb958d4d-1e6b-4677-a89f-3085487b7b49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for xgb optimization: 0.43 min\n",
      "최적의 하이퍼파라미터: OrderedDict([('alpha', 1.0), ('colsample_bytree', 1.0), ('gamma', 0.10966739378542173), ('learning_rate', 1.0), ('max_depth', 1), ('min_child_weight', 10), ('n_estimators', 50), ('reg_lambda', 0.0), ('subsample', 0.10066623913222518)])\n",
      "Elapsed time for xgb training: 0.00 min\n",
      "[XGB 학습 데이터]\n",
      "train_prediction=array([85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8,\n",
      "       85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8,\n",
      "       85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8,\n",
      "       85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8,\n",
      "       85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8,\n",
      "       85.8, 85.8, 85.8, 85.8, 85.8], dtype=float32)\n",
      "result list: [4.8000030517578125, 4.1999969482421875, 5.1999969482421875, 4.1999969482421875, 6.1999969482421875, 8.800003051757812, 11.199996948242188, 4.8000030517578125, 7.1999969482421875, 11.199996948242188, 8.199996948242188, 8.800003051757812, 16.800003051757812, 0.8000030517578125, 25.800003051757812, 14.199996948242188, 4.1999969482421875, 9.199996948242188, 21.800003051757812, 11.800003051757812, 3.8000030517578125, 15.800003051757812, 0.1999969482421875, 35.80000305175781, 12.199996948242188, 23.800003051757812, 7.1999969482421875, 11.199996948242188, 9.199996948242188, 0.1999969482421875, 12.199996948242188, 9.199996948242188, 5.1999969482421875, 2.1999969482421875, 8.199996948242188, 25.800003051757812, 4.1999969482421875, 2.8000030517578125, 1.8000030517578125, 3.8000030517578125, 4.1999969482421875, 4.1999969482421875, 5.1999969482421875, 12.199996948242188, 13.199996948242188, 1.1999969482421875, 0.8000030517578125, 19.800003051757812, 7.1999969482421875, 4.1999969482421875, 5.8000030517578125, 5.1999969482421875, 10.800003051757812, 11.199996948242188, 21.800003051757812, 0.8000030517578125, 10.199996948242188, 25.800003051757812, 4.8000030517578125, 0.8000030517578125]\n",
      "result: 11.919 (std: 7.546)\n",
      "[XGB 검증 데이터]\n",
      "test_prediction=array([85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8, 85.8,\n",
      "       85.8, 85.8, 85.8, 85.8], dtype=float32)\n",
      "result list: [13.199996948242188, 5.1999969482421875, 4.1999969482421875, 4.1999969482421875, 0.1999969482421875, 12.199996948242188, 2.1999969482421875, 0.1999969482421875, 11.199996948242188, 10.800003051757812, 0.1999969482421875, 55.80000305175781, 11.800003051757812, 10.800003051757812, 5.1999969482421875]\n",
      "result: 16.389 (std: 13.116)\n"
     ]
    }
   ],
   "source": [
    "param_space = {\n",
    "    'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "    'max_depth': (1, 50),\n",
    "    'n_estimators': (50, 200),\n",
    "    'min_child_weight': (1, 10),\n",
    "    'subsample': (0.1, 1.0, 'uniform'),\n",
    "    'gamma': (0.1, 5.0),\n",
    "    'colsample_bytree': (0.1, 1.0, 'uniform'),\n",
    "    'alpha': (0.0, 1.0, 'uniform'),\n",
    "    'reg_lambda': (0.0, 1.0, 'uniform')\n",
    "}\n",
    "\n",
    "# 하이퍼 파라미터 튜닝\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "opt = BayesSearchCV(xgb_model, param_space, n_iter=50, cv=10, n_jobs=-1, n_points=8, verbose=0)\n",
    "\n",
    "timer = time.time()\n",
    "opt.fit(X_train, y_train)\n",
    "print(f'Elapsed time for xgb optimization: {(time.time() - timer)/60:0.2f} min')\n",
    "print(\"최적의 하이퍼파라미터:\", opt.best_params_)\n",
    "\n",
    "# XGB 학습\n",
    "timer = time.time()\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', **opt.best_params_)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(f'Elapsed time for xgb training: {(time.time() - timer)/60:0.2f} min')\n",
    "\n",
    "# XGB 학습데이터 추론\n",
    "print('[XGB 학습 데이터]')\n",
    "train_prediction = xgb_model.predict(X_train)\n",
    "print(f'{train_prediction=}')\n",
    "rmse_list = [np.abs(y_train[i] - train_prediction[i]) for i in range(len(y_train))]\n",
    "rmse = np.sqrt(mean_squared_error(y_train, train_prediction))\n",
    "rmse_std = np.std(rmse_list)\n",
    "print(f'result list: {rmse_list}')\n",
    "print(f'result: {rmse:0.3f} (std: {rmse_std:0.3f})')\n",
    "for index in range(train_prediction.shape[0]):\n",
    "    result_table.loc[result_table.shape[0]] = [X_train_index[index], 'train', 'XGB', y_train[index], train_prediction[index], rmse_list[index]]\n",
    "\n",
    "# XGB 추론\n",
    "print('[XGB 검증 데이터]')\n",
    "test_prediction = xgb_model.predict(X_test)\n",
    "print(f'{test_prediction=}')\n",
    "rmse_list = [np.abs(y_test[i] - test_prediction[i]) for i in range(len(y_test))]\n",
    "rmse = np.sqrt(mean_squared_error(y_test, test_prediction))\n",
    "rmse_std = np.std(rmse_list)\n",
    "print(f'result list: {rmse_list}')\n",
    "print(f'result: {rmse:0.3f} (std: {rmse_std:0.3f})')\n",
    "for index in range(test_prediction.shape[0]):\n",
    "    result_table.loc[result_table.shape[0]] = [X_test_index[index], 'test', 'XGB', y_test[index], test_prediction[index], rmse_list[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fab00fa-f020-4622-b399-157db5d23aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">test</th>\n",
       "      <th>LR</th>\n",
       "      <td>10.209833</td>\n",
       "      <td>13.005579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>10.317801</td>\n",
       "      <td>13.048011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>9.591844</td>\n",
       "      <td>14.367333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>9.826665</td>\n",
       "      <td>13.576634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">train</th>\n",
       "      <th>LR</th>\n",
       "      <td>8.016582</td>\n",
       "      <td>6.538506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>9.400648</td>\n",
       "      <td>7.310793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVR</th>\n",
       "      <td>8.541950</td>\n",
       "      <td>8.566745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>9.226666</td>\n",
       "      <td>7.609225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean        std\n",
       "                  rmse       rmse\n",
       "type  model                      \n",
       "test  LR     10.209833  13.005579\n",
       "      RF     10.317801  13.048011\n",
       "      SVR     9.591844  14.367333\n",
       "      XGB     9.826665  13.576634\n",
       "train LR      8.016582   6.538506\n",
       "      RF      9.400648   7.310793\n",
       "      SVR     8.541950   8.566745\n",
       "      XGB     9.226666   7.609225"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table.to_excel('result/result_table_tfidf.xlsx', index=False)\n",
    "\n",
    "pd.pivot_table(result_table,\n",
    "               values=['rmse'],\n",
    "               index=['type', 'model'],\n",
    "               aggfunc=['mean', 'std'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
